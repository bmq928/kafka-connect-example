client.rack = 

connections.max.idle.ms = 540000

default.api.timeout.ms = 60000

enable.auto.commit = false

exclude.internal.topics = true

fetch.max.bytes = 52428800

fetch.max.wait.ms = 500

fetch.min.bytes = 1

group.id = compose-connect-group

group.instance.id = null

heartbeat.interval.ms = 3000

interceptor.classes = []

internal.leave.group.on.close = true

internal.throw.on.fetch.stable.offset.unsupported = false

isolation.level = read_uncommitted

key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

max.partition.fetch.bytes = 1048576

max.poll.interval.ms = 300000

max.poll.records = 500

metadata.max.age.ms = 300000

metric.reporters = []

metrics.num.samples = 2

metrics.recording.level = INFO

metrics.sample.window.ms = 30000

partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]

receive.buffer.bytes = 65536

reconnect.backoff.max.ms = 1000

reconnect.backoff.ms = 50

request.timeout.ms = 30000

retry.backoff.ms = 100

sasl.client.callback.handler.class = null

sasl.jaas.config = null

sasl.kerberos.kinit.cmd = /usr/bin/kinit

sasl.kerberos.min.time.before.relogin = 60000

sasl.kerberos.service.name = null

sasl.kerberos.ticket.renew.jitter = 0.05

sasl.kerberos.ticket.renew.window.factor = 0.8

sasl.login.callback.handler.class = null

sasl.login.class = null

sasl.login.refresh.buffer.seconds = 300

sasl.login.refresh.min.period.seconds = 60

sasl.login.refresh.window.factor = 0.8

sasl.login.refresh.window.jitter = 0.05

sasl.mechanism = GSSAPI

security.protocol = PLAINTEXT

security.providers = null

send.buffer.bytes = 131072

session.timeout.ms = 10000

socket.connection.setup.timeout.max.ms = 30000

socket.connection.setup.timeout.ms = 10000

ssl.cipher.suites = null

ssl.enabled.protocols = [TLSv1.2, TLSv1.3]

ssl.endpoint.identification.algorithm = https

ssl.engine.factory.class = null

ssl.key.password = null

ssl.keymanager.algorithm = SunX509

ssl.keystore.certificate.chain = null

ssl.keystore.key = null

ssl.keystore.location = null

ssl.keystore.password = null

ssl.keystore.type = JKS

ssl.protocol = TLSv1.3

ssl.provider = null

ssl.secure.random.implementation = null

ssl.trustmanager.algorithm = PKIX

ssl.truststore.certificates = null

ssl.truststore.location = null

ssl.truststore.password = null

ssl.truststore.type = JKS

value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

 (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,211] INFO [Producer clientId=producer-2] Cluster ID: ExE0fEKETe2m2uzuIWjKMQ (org.apache.kafka.clients.Metadata)

[2021-10-14 05:31:12,224] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,227] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,227] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,227] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,227] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,227] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,227] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,227] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,227] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,227] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,227] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,228] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,229] INFO Kafka version: 6.2.1-ce (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:31:12,231] INFO Kafka commitId: 14770bfc4e973178 (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:31:12,231] INFO Kafka startTimeMs: 1634189472228 (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:31:12,242] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Cluster ID: ExE0fEKETe2m2uzuIWjKMQ (org.apache.kafka.clients.Metadata)

[2021-10-14 05:31:12,245] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Subscribed to partition(s): docker-connect-status-0, docker-connect-status-1, docker-connect-status-4, docker-connect-status-2, docker-connect-status-3 (org.apache.kafka.clients.consumer.KafkaConsumer)

[2021-10-14 05:31:12,246] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)

[2021-10-14 05:31:12,246] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)

[2021-10-14 05:31:12,246] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)

[2021-10-14 05:31:12,246] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)

[2021-10-14 05:31:12,246] INFO [Consumer clientId=consumer-compose-connect-group-2, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)

[2021-10-14 05:31:12,480] INFO Finished reading KafkaBasedLog for topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)

[2021-10-14 05:31:12,480] INFO Started KafkaBasedLog for topic docker-connect-status (org.apache.kafka.connect.util.KafkaBasedLog)

[2021-10-14 05:31:12,489] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)

[2021-10-14 05:31:12,490] INFO Starting KafkaBasedLog with topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)

[2021-10-14 05:31:12,510] INFO ProducerConfig values: 

acks = -1

batch.size = 16384

bootstrap.servers = [broker:29092]

buffer.memory = 33554432

client.dns.lookup = use_all_dns_ips

client.id = producer-3

compression.type = none

connections.max.idle.ms = 540000

delivery.timeout.ms = 2147483647

enable.idempotence = false

interceptor.classes = []

internal.auto.downgrade.txn.commit = false

key.serializer = class org.apache.kafka.common.serialization.StringSerializer

linger.ms = 0

max.block.ms = 60000

max.in.flight.requests.per.connection = 1

max.request.size = 1048576

metadata.max.age.ms = 300000

metadata.max.idle.ms = 300000

metric.reporters = []

metrics.num.samples = 2

metrics.recording.level = INFO

metrics.sample.window.ms = 30000

partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner

receive.buffer.bytes = 32768

reconnect.backoff.max.ms = 1000

reconnect.backoff.ms = 50

request.timeout.ms = 30000

retries = 2147483647

retry.backoff.ms = 100

sasl.client.callback.handler.class = null

sasl.jaas.config = null

sasl.kerberos.kinit.cmd = /usr/bin/kinit

sasl.kerberos.min.time.before.relogin = 60000

sasl.kerberos.service.name = null

sasl.kerberos.ticket.renew.jitter = 0.05

sasl.kerberos.ticket.renew.window.factor = 0.8

sasl.login.callback.handler.class = null

sasl.login.class = null

sasl.login.refresh.buffer.seconds = 300

sasl.login.refresh.min.period.seconds = 60

sasl.login.refresh.window.factor = 0.8

sasl.login.refresh.window.jitter = 0.05

sasl.mechanism = GSSAPI

security.protocol = PLAINTEXT

security.providers = null

send.buffer.bytes = 131072

socket.connection.setup.timeout.max.ms = 30000

socket.connection.setup.timeout.ms = 10000

ssl.cipher.suites = null

ssl.enabled.protocols = [TLSv1.2, TLSv1.3]

ssl.endpoint.identification.algorithm = https

ssl.engine.factory.class = null

ssl.key.password = null

ssl.keymanager.algorithm = SunX509

ssl.keystore.certificate.chain = null

ssl.keystore.key = null

ssl.keystore.location = null

ssl.keystore.password = null

ssl.keystore.type = JKS

ssl.protocol = TLSv1.3

ssl.provider = null

ssl.secure.random.implementation = null

ssl.trustmanager.algorithm = PKIX

ssl.truststore.certificates = null

ssl.truststore.location = null

ssl.truststore.password = null

ssl.truststore.type = JKS

transaction.timeout.ms = 60000

transactional.id = null

value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

 (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,526] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,526] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,527] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,527] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,527] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,527] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,527] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,527] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,527] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,528] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,528] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,528] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,528] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,529] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,529] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,530] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,530] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,531] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,531] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,533] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,533] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,533] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,533] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:31:12,534] INFO Kafka version: 6.2.1-ce (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:31:12,534] INFO Kafka commitId: 14770bfc4e973178 (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:31:12,535] INFO Kafka startTimeMs: 1634189472534 (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:31:12,536] INFO ConsumerConfig values: 

allow.auto.create.topics = true

auto.commit.interval.ms = 5000

auto.offset.reset = earliest

bootstrap.servers = [broker:29092]

check.crcs = true

client.dns.lookup = use_all_dns_ips

client.id = consumer-compose-connect-group-3

client.rack = 

connections.max.idle.ms = 540000

default.api.timeout.ms = 60000

enable.auto.commit = false

exclude.internal.topics = true

fetch.max.bytes = 52428800

fetch.max.wait.ms = 500

fetch.min.bytes = 1

group.id = compose-connect-group

group.instance.id = null

heartbeat.interval.ms = 3000

interceptor.classes = []

internal.leave.group.on.close = true

internal.throw.on.fetch.stable.offset.unsupported = false

isolation.level = read_uncommitted

key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

max.partition.fetch.bytes = 1048576

max.poll.interval.ms = 300000

max.poll.records = 500

metadata.max.age.ms = 300000

metric.reporters = []

metrics.num.samples = 2

metrics.recording.level = INFO

metrics.sample.window.ms = 30000

partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]

receive.buffer.bytes = 65536

reconnect.backoff.max.ms = 1000

reconnect.backoff.ms = 50

request.timeout.ms = 30000

retry.backoff.ms = 100

sasl.client.callback.handler.class = null

sasl.jaas.config = null

sasl.kerberos.kinit.cmd = /usr/bin/kinit

sasl.kerberos.min.time.before.relogin = 60000

sasl.kerberos.service.name = null

sasl.kerberos.ticket.renew.jitter = 0.05

sasl.kerberos.ticket.renew.window.factor = 0.8

sasl.login.callback.handler.class = null

sasl.login.class = null

sasl.login.refresh.buffer.seconds = 300

sasl.login.refresh.min.period.seconds = 60

sasl.login.refresh.window.factor = 0.8

sasl.login.refresh.window.jitter = 0.05

sasl.mechanism = GSSAPI

security.protocol = PLAINTEXT

security.providers = null

send.buffer.bytes = 131072

session.timeout.ms = 10000

socket.connection.setup.timeout.max.ms = 30000

socket.connection.setup.timeout.ms = 10000

ssl.cipher.suites = null

ssl.enabled.protocols = [TLSv1.2, TLSv1.3]

ssl.endpoint.identification.algorithm = https

ssl.engine.factory.class = null

ssl.key.password = null

ssl.keymanager.algorithm = SunX509

ssl.keystore.certificate.chain = null

ssl.keystore.key = null

ssl.keystore.location = null

ssl.keystore.password = null

ssl.keystore.type = JKS

ssl.protocol = TLSv1.3

ssl.provider = null

ssl.secure.random.implementation = null

ssl.trustmanager.algorithm = PKIX

ssl.truststore.certificates = null

ssl.truststore.location = null

ssl.truststore.password = null

ssl.truststore.type = JKS

value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

 (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,551] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,551] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,552] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,553] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,555] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,559] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,559] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,559] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,559] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,560] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,560] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,560] WARN The configuration 'expose.internal.connect.endpoints' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,560] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,560] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,563] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,563] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,563] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,564] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,564] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,565] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,565] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,565] INFO [Producer clientId=producer-3] Cluster ID: ExE0fEKETe2m2uzuIWjKMQ (org.apache.kafka.clients.Metadata)

[2021-10-14 05:31:12,566] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:31:12,567] INFO Kafka version: 6.2.1-ce (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:31:12,567] INFO Kafka commitId: 14770bfc4e973178 (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:31:12,568] INFO Kafka startTimeMs: 1634189472567 (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:31:12,576] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Cluster ID: ExE0fEKETe2m2uzuIWjKMQ (org.apache.kafka.clients.Metadata)

[2021-10-14 05:31:12,582] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Subscribed to partition(s): docker-connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer)

[2021-10-14 05:31:12,582] INFO [Consumer clientId=consumer-compose-connect-group-3, groupId=compose-connect-group] Seeking to EARLIEST offset of partition docker-connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)

[2021-10-14 05:31:12,616] INFO Successfully processed removal of connector 'JdbcSinkConnectorConnector_0' (org.apache.kafka.connect.storage.KafkaConfigBackingStore)

[2021-10-14 05:31:12,622] INFO Finished reading KafkaBasedLog for topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)

[2021-10-14 05:31:12,629] INFO Started KafkaBasedLog for topic docker-connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)

[2021-10-14 05:31:12,629] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)

[2021-10-14 05:31:12,629] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:31:12,656] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Cluster ID: ExE0fEKETe2m2uzuIWjKMQ (org.apache.kafka.clients.Metadata)

[2021-10-14 05:31:12,657] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:31:12,661] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)

[2021-10-14 05:31:12,661] INFO [Worker clientId=connect-1, groupId=compose-connect-group] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:31:12,680] INFO [Worker clientId=connect-1, groupId=compose-connect-group] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:31:12,688] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Successfully joined group with generation Generation{generationId=7, memberId='connect-1-6d7f375e-7fb8-447b-9900-6ed7ab4bcb20', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:31:12,772] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Successfully synced group in generation Generation{generationId=7, memberId='connect-1-6d7f375e-7fb8-447b-9900-6ed7ab4bcb20', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:31:12,789] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-6d7f375e-7fb8-447b-9900-6ed7ab4bcb20', leaderUrl='http://connect:8083/', offset=6, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:31:12,792] WARN [Worker clientId=connect-1, groupId=compose-connect-group] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:31:12,792] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Current config state offset -1 is behind group assignment 6, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:31:12,805] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Finished reading to end of log and updated config snapshot, new config log offset: 6 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:31:12,805] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Starting connectors and tasks using config offset 6 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:31:12,806] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:31:12,978] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

Oct 14, 2021 5:31:13 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime

WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 

Oct 14, 2021 5:31:14 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime

WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 

Oct 14, 2021 5:31:14 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime

WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 

Oct 14, 2021 5:31:14 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime

WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConfluentV1MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConfluentV1MetadataResource will be ignored. 

Oct 14, 2021 5:31:14 AM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime

WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 

Oct 14, 2021 5:31:17 AM org.glassfish.jersey.internal.Errors logErrors

WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.

WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.

[2021-10-14 05:31:14,959] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)

[2021-10-14 05:31:17,386] INFO Started o.e.j.s.ServletContextHandler@5f2d61cd{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)

[2021-10-14 05:31:17,387] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer)

WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.

WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.

WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.


[2021-10-14 05:31:17,388] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect)

[2021-10-14 05:32:52,888] INFO AbstractConfig values: 

 (org.apache.kafka.common.config.AbstractConfig)

[2021-10-14 05:32:52,967] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Connector JdbcSinkConnectorConnector_0 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:53,075] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)

[2021-10-14 05:32:53,076] INFO [Worker clientId=connect-1, groupId=compose-connect-group] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:53,086] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Successfully joined group with generation Generation{generationId=8, memberId='connect-1-6d7f375e-7fb8-447b-9900-6ed7ab4bcb20', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:53,144] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Successfully synced group in generation Generation{generationId=8, memberId='connect-1-6d7f375e-7fb8-447b-9900-6ed7ab4bcb20', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:53,146] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-6d7f375e-7fb8-447b-9900-6ed7ab4bcb20', leaderUrl='http://connect:8083/', offset=8, connectorIds=[JdbcSinkConnectorConnector_0], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:53,148] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Starting connectors and tasks using config offset 8 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:53,166] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Starting connector JdbcSinkConnectorConnector_0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:53,274] INFO Creating connector JdbcSinkConnectorConnector_0 of type io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker)

[2021-10-14 05:32:53,285] INFO SinkConnectorConfig values: 

config.action.reload = restart

connector.class = io.confluent.connect.jdbc.JdbcSinkConnector

errors.deadletterqueue.context.headers.enable = false

errors.deadletterqueue.topic.name = 

errors.deadletterqueue.topic.replication.factor = 3

errors.log.enable = false

errors.log.include.messages = false

errors.retry.delay.max.ms = 60000

errors.retry.timeout = 0

errors.tolerance = none

header.converter = null

key.converter = null

name = JdbcSinkConnectorConnector_0

predicates = []

tasks.max = 1

topics = [incomming]

topics.regex = 

transforms = []

value.converter = null

 (org.apache.kafka.connect.runtime.SinkConnectorConfig)

[2021-10-14 05:32:53,287] INFO EnrichedConnectorConfig values: 

config.action.reload = restart

connector.class = io.confluent.connect.jdbc.JdbcSinkConnector

errors.deadletterqueue.context.headers.enable = false

errors.deadletterqueue.topic.name = 

errors.deadletterqueue.topic.replication.factor = 3

errors.log.enable = false

errors.log.include.messages = false

errors.retry.delay.max.ms = 60000

errors.retry.timeout = 0

errors.tolerance = none

header.converter = null

key.converter = null

name = JdbcSinkConnectorConnector_0

predicates = []

tasks.max = 1

topics = [incomming]

topics.regex = 

transforms = []

value.converter = null

 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)

[2021-10-14 05:32:53,344] INFO Instantiated connector JdbcSinkConnectorConnector_0 with version 10.2.4 of type class io.confluent.connect.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker)

[2021-10-14 05:32:53,356] INFO Finished creating connector JdbcSinkConnectorConnector_0 (org.apache.kafka.connect.runtime.Worker)

[2021-10-14 05:32:53,359] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:53,518] INFO SinkConnectorConfig values: 

config.action.reload = restart

connector.class = io.confluent.connect.jdbc.JdbcSinkConnector

errors.deadletterqueue.context.headers.enable = false

errors.deadletterqueue.topic.name = 

errors.deadletterqueue.topic.replication.factor = 3

errors.log.enable = false

errors.log.include.messages = false

errors.retry.delay.max.ms = 60000

errors.retry.timeout = 0

errors.tolerance = none

header.converter = null

key.converter = null

name = JdbcSinkConnectorConnector_0

predicates = []

tasks.max = 1

topics = [incomming]

topics.regex = 

transforms = []

value.converter = null

 (org.apache.kafka.connect.runtime.SinkConnectorConfig)

[2021-10-14 05:32:53,518] INFO EnrichedConnectorConfig values: 

config.action.reload = restart

connector.class = io.confluent.connect.jdbc.JdbcSinkConnector

errors.deadletterqueue.context.headers.enable = false

errors.deadletterqueue.topic.name = 

errors.deadletterqueue.topic.replication.factor = 3

errors.log.enable = false

errors.log.include.messages = false

errors.retry.delay.max.ms = 60000

errors.retry.timeout = 0

errors.tolerance = none

header.converter = null

key.converter = null

name = JdbcSinkConnectorConnector_0

predicates = []

tasks.max = 1

topics = [incomming]

topics.regex = 

transforms = []

value.converter = null

 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)

[2021-10-14 05:32:53,523] INFO Setting task configurations for 1 workers. (io.confluent.connect.jdbc.JdbcSinkConnector)

[2021-10-14 05:32:53,571] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Tasks [JdbcSinkConnectorConnector_0-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:53,574] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:53,575] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)

[2021-10-14 05:32:53,575] INFO [Worker clientId=connect-1, groupId=compose-connect-group] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:53,583] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Successfully joined group with generation Generation{generationId=9, memberId='connect-1-6d7f375e-7fb8-447b-9900-6ed7ab4bcb20', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:53,590] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Successfully synced group in generation Generation{generationId=9, memberId='connect-1-6d7f375e-7fb8-447b-9900-6ed7ab4bcb20', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:53,590] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-6d7f375e-7fb8-447b-9900-6ed7ab4bcb20', leaderUrl='http://connect:8083/', offset=10, connectorIds=[JdbcSinkConnectorConnector_0], taskIds=[JdbcSinkConnectorConnector_0-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:53,592] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Starting connectors and tasks using config offset 10 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:53,598] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Starting task JdbcSinkConnectorConnector_0-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:53,604] INFO Creating task JdbcSinkConnectorConnector_0-0 (org.apache.kafka.connect.runtime.Worker)

[2021-10-14 05:32:53,606] INFO ConnectorConfig values: 

config.action.reload = restart

connector.class = io.confluent.connect.jdbc.JdbcSinkConnector

errors.log.enable = false

errors.log.include.messages = false

errors.retry.delay.max.ms = 60000

errors.retry.timeout = 0

errors.tolerance = none

header.converter = null

key.converter = null

name = JdbcSinkConnectorConnector_0

predicates = []

tasks.max = 1

transforms = []

value.converter = null

 (org.apache.kafka.connect.runtime.ConnectorConfig)

[2021-10-14 05:32:53,607] INFO EnrichedConnectorConfig values: 

config.action.reload = restart

connector.class = io.confluent.connect.jdbc.JdbcSinkConnector

errors.log.enable = false

errors.log.include.messages = false

errors.retry.delay.max.ms = 60000

errors.retry.timeout = 0

errors.tolerance = none

header.converter = null

key.converter = null

name = JdbcSinkConnectorConnector_0

predicates = []

tasks.max = 1

transforms = []

value.converter = null

 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)

[2021-10-14 05:32:53,619] INFO TaskConfig values: 

task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask

 (org.apache.kafka.connect.runtime.TaskConfig)

[2021-10-14 05:32:53,620] INFO Instantiated task JdbcSinkConnectorConnector_0-0 with version 10.2.4 of type io.confluent.connect.jdbc.sink.JdbcSinkTask (org.apache.kafka.connect.runtime.Worker)

[2021-10-14 05:32:53,622] INFO StringConverterConfig values: 

converter.encoding = UTF8

converter.type = key

 (org.apache.kafka.connect.storage.StringConverterConfig)

[2021-10-14 05:32:53,623] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task JdbcSinkConnectorConnector_0-0 using the worker config (org.apache.kafka.connect.runtime.Worker)

[2021-10-14 05:32:53,639] INFO AvroConverterConfig values: 

auto.register.schemas = true

basic.auth.credentials.source = URL

basic.auth.user.info = [hidden]

bearer.auth.credentials.source = STATIC_TOKEN

bearer.auth.token = [hidden]

key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

latest.compatibility.strict = true

max.schemas.per.subject = 1000

proxy.host = 

proxy.port = -1

schema.reflection = false

schema.registry.basic.auth.user.info = [hidden]

schema.registry.ssl.cipher.suites = null

schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]

schema.registry.ssl.endpoint.identification.algorithm = https

schema.registry.ssl.engine.factory.class = null

schema.registry.ssl.key.password = null

schema.registry.ssl.keymanager.algorithm = SunX509

schema.registry.ssl.keystore.certificate.chain = null

schema.registry.ssl.keystore.key = null

schema.registry.ssl.keystore.location = null

schema.registry.ssl.keystore.password = null

schema.registry.ssl.keystore.type = JKS

schema.registry.ssl.protocol = TLSv1.3

schema.registry.ssl.provider = null

schema.registry.ssl.secure.random.implementation = null

schema.registry.ssl.trustmanager.algorithm = PKIX

schema.registry.ssl.truststore.certificates = null

schema.registry.ssl.truststore.location = null

schema.registry.ssl.truststore.password = null

schema.registry.ssl.truststore.type = JKS

schema.registry.url = [http://schema-registry:8081]

use.latest.version = false

value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

 (io.confluent.connect.avro.AvroConverterConfig)

[2021-10-14 05:32:53,741] INFO KafkaAvroSerializerConfig values: 

auto.register.schemas = true

avro.reflection.allow.null = false

avro.remove.java.properties = false

avro.use.logical.type.converters = false

basic.auth.credentials.source = URL

basic.auth.user.info = [hidden]

bearer.auth.credentials.source = STATIC_TOKEN

bearer.auth.token = [hidden]

key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

latest.compatibility.strict = true

max.schemas.per.subject = 1000

proxy.host = 

proxy.port = -1

schema.reflection = false

schema.registry.basic.auth.user.info = [hidden]

schema.registry.ssl.cipher.suites = null

schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]

schema.registry.ssl.endpoint.identification.algorithm = https

schema.registry.ssl.engine.factory.class = null

schema.registry.ssl.key.password = null

schema.registry.ssl.keymanager.algorithm = SunX509

schema.registry.ssl.keystore.certificate.chain = null

schema.registry.ssl.keystore.key = null

schema.registry.ssl.keystore.location = null

schema.registry.ssl.keystore.password = null

schema.registry.ssl.keystore.type = JKS

schema.registry.ssl.protocol = TLSv1.3

schema.registry.ssl.provider = null

schema.registry.ssl.secure.random.implementation = null

schema.registry.ssl.trustmanager.algorithm = PKIX

schema.registry.ssl.truststore.certificates = null

schema.registry.ssl.truststore.location = null

schema.registry.ssl.truststore.password = null

schema.registry.ssl.truststore.type = JKS

schema.registry.url = [http://schema-registry:8081]

use.latest.version = false

value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

 (io.confluent.kafka.serializers.KafkaAvroSerializerConfig)

[2021-10-14 05:32:53,759] INFO KafkaAvroDeserializerConfig values: 

auto.register.schemas = true

avro.reflection.allow.null = false

avro.use.logical.type.converters = false

basic.auth.credentials.source = URL

basic.auth.user.info = [hidden]

bearer.auth.credentials.source = STATIC_TOKEN

bearer.auth.token = [hidden]

key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

latest.compatibility.strict = true

max.schemas.per.subject = 1000

proxy.host = 

proxy.port = -1

schema.reflection = false

schema.registry.basic.auth.user.info = [hidden]

schema.registry.ssl.cipher.suites = null

schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]

schema.registry.ssl.endpoint.identification.algorithm = https

schema.registry.ssl.engine.factory.class = null

schema.registry.ssl.key.password = null

schema.registry.ssl.keymanager.algorithm = SunX509

schema.registry.ssl.keystore.certificate.chain = null

schema.registry.ssl.keystore.key = null

schema.registry.ssl.keystore.location = null

schema.registry.ssl.keystore.password = null

schema.registry.ssl.keystore.type = JKS

schema.registry.ssl.protocol = TLSv1.3

schema.registry.ssl.provider = null

schema.registry.ssl.secure.random.implementation = null

schema.registry.ssl.trustmanager.algorithm = PKIX

schema.registry.ssl.truststore.certificates = null

schema.registry.ssl.truststore.location = null

schema.registry.ssl.truststore.password = null

schema.registry.ssl.truststore.type = JKS

schema.registry.url = [http://schema-registry:8081]

specific.avro.reader = false

use.latest.version = false

value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

 (io.confluent.kafka.serializers.KafkaAvroDeserializerConfig)

[2021-10-14 05:32:53,877] INFO AvroDataConfig values: 

connect.meta.data = true

enhanced.avro.schema.support = false

schemas.cache.config = 1000

scrub.invalid.names = false

 (io.confluent.connect.avro.AvroDataConfig)

[2021-10-14 05:32:53,877] INFO Set up the value converter class io.confluent.connect.avro.AvroConverter for task JdbcSinkConnectorConnector_0-0 using the worker config (org.apache.kafka.connect.runtime.Worker)

[2021-10-14 05:32:53,878] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task JdbcSinkConnectorConnector_0-0 using the worker config (org.apache.kafka.connect.runtime.Worker)

[2021-10-14 05:32:53,909] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker)

[2021-10-14 05:32:53,909] INFO SinkConnectorConfig values: 

config.action.reload = restart

connector.class = io.confluent.connect.jdbc.JdbcSinkConnector

errors.deadletterqueue.context.headers.enable = false

errors.deadletterqueue.topic.name = 

errors.deadletterqueue.topic.replication.factor = 3

errors.log.enable = false

errors.log.include.messages = false

errors.retry.delay.max.ms = 60000

errors.retry.timeout = 0

errors.tolerance = none

header.converter = null

key.converter = null

name = JdbcSinkConnectorConnector_0

predicates = []

tasks.max = 1

topics = [incomming]

topics.regex = 

transforms = []

value.converter = null

 (org.apache.kafka.connect.runtime.SinkConnectorConfig)

[2021-10-14 05:32:53,914] INFO EnrichedConnectorConfig values: 

config.action.reload = restart

connector.class = io.confluent.connect.jdbc.JdbcSinkConnector

errors.deadletterqueue.context.headers.enable = false

errors.deadletterqueue.topic.name = 

errors.deadletterqueue.topic.replication.factor = 3

errors.log.enable = false

errors.log.include.messages = false

errors.retry.delay.max.ms = 60000

errors.retry.timeout = 0

errors.tolerance = none

header.converter = null

key.converter = null

name = JdbcSinkConnectorConnector_0

predicates = []

tasks.max = 1

topics = [incomming]

topics.regex = 

transforms = []

value.converter = null

 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)

[2021-10-14 05:32:53,942] INFO ConsumerConfig values: 

allow.auto.create.topics = true

auto.commit.interval.ms = 5000

auto.offset.reset = earliest

bootstrap.servers = [broker:29092]

check.crcs = true

client.dns.lookup = use_all_dns_ips

client.id = connector-consumer-JdbcSinkConnectorConnector_0-0

client.rack = 

connections.max.idle.ms = 540000

default.api.timeout.ms = 60000

enable.auto.commit = false

exclude.internal.topics = true

fetch.max.bytes = 52428800

fetch.max.wait.ms = 500

fetch.min.bytes = 1

group.id = connect-JdbcSinkConnectorConnector_0

group.instance.id = null

heartbeat.interval.ms = 3000

interceptor.classes = [io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor]

internal.leave.group.on.close = true

internal.throw.on.fetch.stable.offset.unsupported = false

isolation.level = read_uncommitted

key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

max.partition.fetch.bytes = 1048576

max.poll.interval.ms = 300000

max.poll.records = 500

metadata.max.age.ms = 300000

metric.reporters = []

metrics.num.samples = 2

metrics.recording.level = INFO

metrics.sample.window.ms = 30000

partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]

receive.buffer.bytes = 65536

reconnect.backoff.max.ms = 1000

reconnect.backoff.ms = 50

request.timeout.ms = 30000

retry.backoff.ms = 100

sasl.client.callback.handler.class = null

sasl.jaas.config = null

sasl.kerberos.kinit.cmd = /usr/bin/kinit

sasl.kerberos.min.time.before.relogin = 60000

sasl.kerberos.service.name = null

sasl.kerberos.ticket.renew.jitter = 0.05

sasl.kerberos.ticket.renew.window.factor = 0.8

sasl.login.callback.handler.class = null

sasl.login.class = null

sasl.login.refresh.buffer.seconds = 300

sasl.login.refresh.min.period.seconds = 60

sasl.login.refresh.window.factor = 0.8

sasl.login.refresh.window.jitter = 0.05

sasl.mechanism = GSSAPI

security.protocol = PLAINTEXT

security.providers = null

send.buffer.bytes = 131072

session.timeout.ms = 10000

socket.connection.setup.timeout.max.ms = 30000

socket.connection.setup.timeout.ms = 10000

ssl.cipher.suites = null

ssl.enabled.protocols = [TLSv1.2, TLSv1.3]

ssl.endpoint.identification.algorithm = https

ssl.engine.factory.class = null

ssl.key.password = null

ssl.keymanager.algorithm = SunX509

ssl.keystore.certificate.chain = null

ssl.keystore.key = null

ssl.keystore.location = null

ssl.keystore.password = null

ssl.keystore.type = JKS

ssl.protocol = TLSv1.3

ssl.provider = null

ssl.secure.random.implementation = null

ssl.trustmanager.algorithm = PKIX

ssl.truststore.certificates = null

ssl.truststore.location = null

ssl.truststore.password = null

ssl.truststore.type = JKS

value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

 (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:32:54,012] WARN The configuration 'metrics.context.resource.connector' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:32:54,014] WARN The configuration 'metrics.context.resource.version' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:32:54,014] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:32:54,014] WARN The configuration 'metrics.context.resource.type' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:32:54,014] WARN The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:32:54,014] WARN The configuration 'metrics.context.resource.task' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:32:54,014] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)

[2021-10-14 05:32:54,014] INFO Kafka version: 6.2.1-ce (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:32:54,014] INFO Kafka commitId: 14770bfc4e973178 (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:32:54,014] INFO Kafka startTimeMs: 1634189574014 (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:32:54,139] INFO [Worker clientId=connect-1, groupId=compose-connect-group] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)

[2021-10-14 05:32:54,193] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Subscribed to topic(s): incomming (org.apache.kafka.clients.consumer.KafkaConsumer)

[2021-10-14 05:32:54,202] INFO Starting JDBC Sink task (io.confluent.connect.jdbc.sink.JdbcSinkTask)

[2021-10-14 05:32:54,206] INFO JdbcSinkConfig values: 

auto.create = false

auto.evolve = false

batch.size = 3000

connection.attempts = 3

connection.backoff.ms = 10000

connection.password = [hidden]

connection.url = jdbc:postgresql://postgres:5432/pg

connection.user = pg

db.timezone = UTC

delete.enabled = false

dialect.name = PostgreSqlDatabaseDialect

fields.whitelist = []

insert.mode = insert

max.retries = 10

pk.fields = []

pk.mode = none

quote.sql.identifiers = ALWAYS

retry.backoff.ms = 3000

table.name.format = ${topic}

table.types = [TABLE]

 (io.confluent.connect.jdbc.sink.JdbcSinkConfig)

[2021-10-14 05:32:54,332] INFO Initializing writer using SQL dialect: PostgreSqlDatabaseDialect (io.confluent.connect.jdbc.sink.JdbcSinkTask)

[2021-10-14 05:32:54,336] INFO WorkerSinkTask{id=JdbcSinkConnectorConnector_0-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask)

[2021-10-14 05:32:54,345] INFO WorkerSinkTask{id=JdbcSinkConnectorConnector_0-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask)

[2021-10-14 05:32:54,368] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Cluster ID: ExE0fEKETe2m2uzuIWjKMQ (org.apache.kafka.clients.Metadata)

[2021-10-14 05:32:54,369] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:54,373] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:54,392] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:54,399] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-JdbcSinkConnectorConnector_0-0-0035545f-53ce-4719-a33e-66320ebdf8b6', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:54,409] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Finished assignment for group at generation 1: {connector-consumer-JdbcSinkConnectorConnector_0-0-0035545f-53ce-4719-a33e-66320ebdf8b6=Assignment(partitions=[incomming-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)

[2021-10-14 05:32:54,425] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-JdbcSinkConnectorConnector_0-0-0035545f-53ce-4719-a33e-66320ebdf8b6', protocol='range'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:54,426] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Notifying assignor about the new Assignment(partitions=[incomming-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)

[2021-10-14 05:32:54,427] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Adding newly assigned partitions: incomming-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)

[2021-10-14 05:32:54,442] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Found no committed offset for partition incomming-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)

[2021-10-14 05:32:54,520] INFO creating interceptor (io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor)

[2021-10-14 05:32:54,680] INFO MonitoringInterceptorConfig values: 

confluent.monitoring.interceptor.publishMs = 15000

confluent.monitoring.interceptor.topic = _confluent-monitoring

 (io.confluent.monitoring.clients.interceptor.MonitoringInterceptorConfig)

[2021-10-14 05:32:54,841] INFO ProducerConfig values: 

acks = -1

batch.size = 16384

bootstrap.servers = [broker:29092]

buffer.memory = 33554432

client.dns.lookup = use_all_dns_ips

client.id = confluent.monitoring.interceptor.connector-consumer-JdbcSinkConnectorConnector_0-0

compression.type = lz4

connections.max.idle.ms = 540000

delivery.timeout.ms = 120000

enable.idempotence = false

interceptor.classes = []

internal.auto.downgrade.txn.commit = false

key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

linger.ms = 500

max.block.ms = 60000

max.in.flight.requests.per.connection = 1

max.request.size = 10485760

metadata.max.age.ms = 300000

metadata.max.idle.ms = 300000

metric.reporters = []

metrics.num.samples = 2

metrics.recording.level = INFO

metrics.sample.window.ms = 30000

partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner

receive.buffer.bytes = 32768

reconnect.backoff.max.ms = 1000

reconnect.backoff.ms = 50

request.timeout.ms = 30000

retries = 2147483647

retry.backoff.ms = 500

sasl.client.callback.handler.class = null

sasl.jaas.config = null

sasl.kerberos.kinit.cmd = /usr/bin/kinit

sasl.kerberos.min.time.before.relogin = 60000

sasl.kerberos.service.name = null

sasl.kerberos.ticket.renew.jitter = 0.05

sasl.kerberos.ticket.renew.window.factor = 0.8

sasl.login.callback.handler.class = null

sasl.login.class = null

sasl.login.refresh.buffer.seconds = 300

sasl.login.refresh.min.period.seconds = 60

sasl.login.refresh.window.factor = 0.8

sasl.login.refresh.window.jitter = 0.05

sasl.mechanism = GSSAPI

security.protocol = PLAINTEXT

security.providers = null

send.buffer.bytes = 131072

socket.connection.setup.timeout.max.ms = 30000

socket.connection.setup.timeout.ms = 10000

ssl.cipher.suites = null

ssl.enabled.protocols = [TLSv1.2, TLSv1.3]

ssl.endpoint.identification.algorithm = https

ssl.engine.factory.class = null

ssl.key.password = null

ssl.keymanager.algorithm = SunX509

ssl.keystore.certificate.chain = null

ssl.keystore.key = null

ssl.keystore.location = null

ssl.keystore.password = null

ssl.keystore.type = JKS

ssl.protocol = TLSv1.3

ssl.provider = null

ssl.secure.random.implementation = null

ssl.trustmanager.algorithm = PKIX

ssl.truststore.certificates = null

ssl.truststore.location = null

ssl.truststore.password = null

ssl.truststore.type = JKS

transaction.timeout.ms = 60000

transactional.id = null

value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

 (org.apache.kafka.clients.producer.ProducerConfig)

[2021-10-14 05:32:54,876] INFO Kafka version: 6.2.1-ce (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:32:54,877] INFO Kafka commitId: 14770bfc4e973178 (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:32:54,877] INFO Kafka startTimeMs: 1634189574875 (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:32:54,959] INFO [Producer clientId=confluent.monitoring.interceptor.connector-consumer-JdbcSinkConnectorConnector_0-0] Cluster ID: ExE0fEKETe2m2uzuIWjKMQ (org.apache.kafka.clients.Metadata)

[2021-10-14 05:32:54,971] INFO interceptor=confluent.monitoring.interceptor.connector-consumer-JdbcSinkConnectorConnector_0-0 created for client_id=connector-consumer-JdbcSinkConnectorConnector_0-0 client_type=CONSUMER session= cluster=ExE0fEKETe2m2uzuIWjKMQ group=connect-JdbcSinkConnectorConnector_0 (io.confluent.monitoring.clients.interceptor.MonitoringInterceptor)

[2021-10-14 05:32:56,925] ERROR WorkerSinkTask{id=JdbcSinkConnectorConnector_0-0} Error converting message value in topic 'incomming' partition 0 at offset 0 and timestamp 1634188485881: Failed to deserialize data for topic incomming to Avro:  (org.apache.kafka.connect.runtime.WorkerSinkTask)

org.apache.kafka.connect.errors.DataException: Failed to deserialize data for topic incomming to Avro: 

Caused by: org.apache.kafka.common.errors.SerializationException: Error retrieving Avro value schema version for id 1

Caused by: io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Subject 'incomming-value' not found.; error code: 40401

[2021-10-14 05:32:56,970] ERROR WorkerSinkTask{id=JdbcSinkConnectorConnector_0-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask)

org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler

Caused by: org.apache.kafka.connect.errors.DataException: Failed to deserialize data for topic incomming to Avro: 

Caused by: org.apache.kafka.common.errors.SerializationException: Error retrieving Avro value schema version for id 1

Caused by: io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Subject 'incomming-value' not found.; error code: 40401


[2021-10-14 05:32:56,973] INFO Stopping task (io.confluent.connect.jdbc.sink.JdbcSinkTask)

[2021-10-14 05:32:57,015] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Revoke previously assigned partitions incomming-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)

[2021-10-14 05:32:57,029] INFO [Consumer clientId=connector-consumer-JdbcSinkConnectorConnector_0-0, groupId=connect-JdbcSinkConnectorConnector_0] Member connector-consumer-JdbcSinkConnectorConnector_0-0-0035545f-53ce-4719-a33e-66320ebdf8b6 sending LeaveGroup request to coordinator broker:29092 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)

[2021-10-14 05:32:57,108] INFO Publish thread interrupted for client_id=connector-consumer-JdbcSinkConnectorConnector_0-0 client_type=CONSUMER session= cluster=ExE0fEKETe2m2uzuIWjKMQ group=connect-JdbcSinkConnectorConnector_0 (io.confluent.monitoring.clients.interceptor.MonitoringInterceptor)

[2021-10-14 05:32:57,455] INFO Publishing Monitoring Metrics stopped for client_id=connector-consumer-JdbcSinkConnectorConnector_0-0 client_type=CONSUMER session= cluster=ExE0fEKETe2m2uzuIWjKMQ group=connect-JdbcSinkConnectorConnector_0 (io.confluent.monitoring.clients.interceptor.MonitoringInterceptor)

[2021-10-14 05:32:57,455] INFO [Producer clientId=confluent.monitoring.interceptor.connector-consumer-JdbcSinkConnectorConnector_0-0] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)

[2021-10-14 05:32:57,530] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)

[2021-10-14 05:32:57,532] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)

[2021-10-14 05:32:57,532] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)

[2021-10-14 05:32:57,536] INFO App info kafka.producer for confluent.monitoring.interceptor.connector-consumer-JdbcSinkConnectorConnector_0-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)

[2021-10-14 05:32:57,536] INFO Closed monitoring interceptor for client_id=connector-consumer-JdbcSinkConnectorConnector_0-0 client_type=CONSUMER session= cluster=ExE0fEKETe2m2uzuIWjKMQ group=connect-JdbcSinkConnectorConnector_0 (io.confluent.monitoring.clients.interceptor.MonitoringInterceptor)

[2021-10-14 05:32:57,537] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)

[2021-10-14 05:32:57,538] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)

[2021-10-14 05:32:57,541] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)

[2021-10-14 05:32:57,559] INFO App info kafka.consumer for connector-consumer-JdbcSinkConnectorConnector_0-0 unregistered (org.apache.kafka.common.utils.AppInfoParser)

